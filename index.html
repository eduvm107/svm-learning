<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>M√°quinas de Vectores de Soporte - Gu√≠a Interactiva</title>
    <link rel="stylesheet" href="style.css">
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <!-- Navbar -->
    <nav class="navbar">
        <div class="nav-container">
            <h1 class="nav-logo">üìä SVM Learning</h1>
            <button class="nav-toggle" id="navToggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            <ul class="nav-menu" id="navMenu">
                <li><a href="#intro" class="nav-link">Introducci√≥n</a></li>
                <li><a href="#maximal" class="nav-link">Margen M√°ximo</a></li>
                <li><a href="#svc" class="nav-link">SVC</a></li>
                <li><a href="#svm" class="nav-link">SVM</a></li>
                <li><a href="#multiclass" class="nav-link">Multiclase</a></li>
                <li><a href="#ejemplos" class="nav-link">Ejemplos</a></li>
            </ul>
        </div>
    </nav>

    <!-- Hero Section -->
    <section class="hero">
        <div class="hero-content">
            <div class="hero-badge">Machine Learning</div>
            <h1 class="hero-title">Support Vector Machines</h1>
            <p class="hero-subtitle">Domina uno de los algoritmos m√°s poderosos de clasificaci√≥n en Machine Learning</p>
            <p class="hero-description">
                Aprende desde los fundamentos matem√°ticos hasta la implementaci√≥n pr√°ctica con visualizaciones interactivas en 3D
            </p>
            <div class="hero-stats">
                <div class="stat-card">
                    <span class="stat-number">3</span>
                    <span class="stat-label">Tipos de Clasificadores</span>
                </div>
                <div class="stat-card">
                    <span class="stat-number">5+</span>
                    <span class="stat-label">Visualizaciones 3D</span>
                </div>
                <div class="stat-card">
                    <span class="stat-number">10+</span>
                    <span class="stat-label">Ejemplos Pr√°cticos</span>
                </div>
            </div>
            <div class="hero-cta">
                <a href="#intro" class="btn-hero-primary">Comenzar Aprendizaje</a>
                <a href="#ejemplos" class="btn-hero-secondary">Ver Demos</a>
            </div>
        </div>
        <div class="hero-animation">
            <div class="floating-shape shape-1"></div>
            <div class="floating-shape shape-2"></div>
            <div class="floating-shape shape-3"></div>
        </div>
    </section>

    <!-- Introducci√≥n -->
    <section id="intro" class="section">
        <div class="container">
            <h2 class="section-title">¬øQu√© son las Support Vector Machines?</h2>
            <div class="content-grid">
                <div class="content-text">
                    <p class="intro-text">
                        Las <strong>Support Vector Machines (SVM)</strong> son un enfoque para clasificaci√≥n desarrollado
                        en la d√©cada de 1990 que ha demostrado un excelente desempe√±o en una variedad de contextos.
                        Son consideradas uno de los mejores clasificadores "listos para usar".
                    </p>

                    <div class="key-points">
                        <h3>üéØ Conceptos Clave</h3>
                        <ul class="key-list">
                            <li><strong>Hiperplano:</strong> Un subespacio plano de dimensi√≥n p-1 que separa el espacio en dos mitades</li>
                            <li><strong>Margen:</strong> La distancia m√≠nima desde las observaciones al hiperplano</li>
                            <li><strong>Vectores de Soporte:</strong> Observaciones que definen el margen m√°ximo</li>
                            <li><strong>Kernel:</strong> Funci√≥n que permite fronteras de decisi√≥n no lineales</li>
                        </ul>
                    </div>

                    <div class="evolution-box">
                        <h3>üìà Evoluci√≥n del M√©todo</h3>
                        <div class="evolution-steps">
                            <div class="step">
                                <span class="step-number">1</span>
                                <p><strong>Clasificador de Margen M√°ximo</strong><br>
                                Simple pero requiere clases linealmente separables</p>
                            </div>
                            <div class="step-arrow">‚Üí</div>
                            <div class="step">
                                <span class="step-number">2</span>
                                <p><strong>Support Vector Classifier</strong><br>
                                Permite violaciones al margen con "soft margin"</p>
                            </div>
                            <div class="step-arrow">‚Üí</div>
                            <div class="step">
                                <span class="step-number">3</span>
                                <p><strong>Support Vector Machine</strong><br>
                                Usa kernels para fronteras no lineales</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Hiperplanos -->
    <section class="section bg-light">
        <div class="container">
            <h2 class="section-title">Entendiendo los Hiperplanos</h2>

            <div class="two-column">
                <div class="column">
                    <h3>Definici√≥n Matem√°tica</h3>
                    <p>En un espacio p-dimensional, un hiperplano se define por:</p>
                    <div class="formula-box">
                        $$\beta_0 + \beta_1X_1 + \beta_2X_2 + \cdots + \beta_pX_p = 0$$
                    </div>
                    <p>Si un punto \(X = (X_1, X_2, \ldots, X_p)^T\) satisface esta ecuaci√≥n, entonces \(X\) se encuentra exactamente en el hiperplano. Esta ecuaci√≥n representa un subespacio af√≠n de dimensi√≥n \(p-1\) en un espacio \(p\)-dimensional.</p>

                    <h4>Propiedades Importantes:</h4>
                    <ul class="property-list">
                        <li><strong>Clasificaci√≥n por signo:</strong> Si \(\beta_0 + \beta_1X_1 + \cdots + \beta_pX_p > 0\), entonces \(X\) est√° en un lado del hiperplano (clase +1)</li>
                        <li><strong>Clasificaci√≥n opuesta:</strong> Si \(\beta_0 + \beta_1X_1 + \cdots + \beta_pX_p < 0\), entonces \(X\) est√° en el otro lado (clase -1)</li>
                        <li><strong>Divisi√≥n del espacio:</strong> El hiperplano divide el espacio p-dimensional en dos semi-espacios disjuntos</li>
                        <li><strong>Vector normal:</strong> El vector \(\beta = (\beta_1, \ldots, \beta_p)^T\) es perpendicular al hiperplano</li>
                        <li><strong>Distancia:</strong> La distancia de un punto al hiperplano es \(\frac{|\beta_0 + \beta^TX|}{\|\beta\|}\)</li>
                    </ul>

                    <div class="info-box">
                        <h4>üí° Intuici√≥n Geom√©trica:</h4>
                        <p>En 2D, un hiperplano es una l√≠nea. En 3D, es un plano. En dimensiones superiores, seguimos llam√°ndolo hiperplano aunque no podamos visualizarlo directamente.</p>
                    </div>
                </div>

                <div class="column">
                    <div class="interactive-box">
                        <h3>Visualizaci√≥n Interactiva 2D</h3>
                        <div id="hyperplane-plot" class="plot-container"></div>
                        <div class="controls">
                            <label>
                                Œ≤‚ÇÄ: <input type="range" id="beta0" min="-5" max="5" step="0.1" value="1">
                                <span id="beta0-value">1</span>
                            </label>
                            <label>
                                Œ≤‚ÇÅ: <input type="range" id="beta1" min="-5" max="5" step="0.1" value="2">
                                <span id="beta1-value">2</span>
                            </label>
                            <label>
                                Œ≤‚ÇÇ: <input type="range" id="beta2" min="-5" max="5" step="0.1" value="3">
                                <span id="beta2-value">3</span>
                            </label>
                        </div>
                        <p class="formula-display">Ecuaci√≥n actual: <span id="current-equation">1 + 2X‚ÇÅ + 3X‚ÇÇ = 0</span></p>
                    </div>
                </div>
            </div>

            <!-- Visualizaci√≥n 3D del Hiperplano -->
            <div class="interactive-box" style="margin-top: 3rem;">
                <h3>üé® Visualizaci√≥n 3D del Hiperplano</h3>
                <p>Esta visualizaci√≥n muestra c√≥mo un hiperplano (en este caso un plano en 3D) separa el espacio tridimensional en dos regiones.</p>
                <div id="hyperplane-3d-plot" class="plot-container-large"></div>
                <div class="controls">
                    <label>
                        Œ≤‚ÇÄ (intercepto): <input type="range" id="beta0-3d" min="-5" max="5" step="0.1" value="0">
                        <span id="beta0-3d-value">0</span>
                    </label>
                    <label>
                        Œ≤‚ÇÅ (coef. X‚ÇÅ): <input type="range" id="beta1-3d" min="-5" max="5" step="0.1" value="1">
                        <span id="beta1-3d-value">1</span>
                    </label>
                    <label>
                        Œ≤‚ÇÇ (coef. X‚ÇÇ): <input type="range" id="beta2-3d" min="-5" max="5" step="0.1" value="1">
                        <span id="beta2-3d-value">1</span>
                    </label>
                    <label>
                        Œ≤‚ÇÉ (coef. X‚ÇÉ): <input type="range" id="beta3-3d" min="-5" max="5" step="0.1" value="1">
                        <span id="beta3-3d-value">1</span>
                    </label>
                </div>
                <p class="formula-display">Ecuaci√≥n del hiperplano: <span id="equation-3d">0 + 1X‚ÇÅ + 1X‚ÇÇ + 1X‚ÇÉ = 0</span></p>
            </div>
        </div>
    </section>

    <!-- Clasificador de Margen M√°ximo -->
    <section id="maximal" class="section">
        <div class="container">
            <h2 class="section-title">Clasificador de Margen M√°ximo</h2>

            <div class="concept-intro">
                <p class="intro-text">
                    Cuando las clases son <strong>linealmente separables</strong>, existen infinitos hiperplanos
                    que pueden separarlas perfectamente. El <em>clasificador de margen m√°ximo</em> elige
                    el hiperplano que est√° m√°s alejado de las observaciones de entrenamiento.
                </p>
                <p class="intro-text">
                    La idea fundamental es que un clasificador basado en un hiperplano que se encuentra muy alejado
                    de los datos de entrenamiento tiene menos probabilidad de cometer errores en nuevas observaciones
                    (datos de prueba). Esta es una aplicaci√≥n del principio de <strong>margin maximization</strong>,
                    que busca crear un "colch√≥n" de separaci√≥n entre las clases.
                </p>
            </div>

            <div class="key-points">
                <h3>üîë Conceptos Fundamentales del Margen M√°ximo</h3>
                <ul class="key-list">
                    <li><strong>Margen (M):</strong> Es la distancia perpendicular m√≠nima desde cualquier observaci√≥n de entrenamiento al hiperplano de separaci√≥n</li>
                    <li><strong>Margen m√°ximo:</strong> El clasificador busca el hiperplano que maximiza esta distancia M</li>
                    <li><strong>Clasificaci√≥n:</strong> Para una observaci√≥n \(x^* = (x_1^*, \ldots, x_p^*)^T\), se predice \(y^* = \text{sign}(f(x^*))\) donde \(f(x^*) = \beta_0 + \beta_1x_1^* + \cdots + \beta_px_p^*\)</li>
                    <li><strong>Confianza:</strong> Cuanto mayor es \(|f(x^*)|\), m√°s confiamos en la clasificaci√≥n</li>
                </ul>
            </div>

            <div class="two-column">
                <div class="column">
                    <h3>Problema de Optimizaci√≥n</h3>
                    <div class="formula-box">
                        $$\text{maximizar}_{\beta_0,\beta_1,\ldots,\beta_p, M} \quad M$$
                        $$\text{sujeto a} \quad \sum_{j=1}^p \beta_j^2 = 1$$
                        $$y_i(\beta_0 + \beta_1x_{i1} + \cdots + \beta_px_{ip}) \geq M, \quad \forall i$$
                    </div>

                    <div class="info-box">
                        <h4>üìù Interpretaci√≥n:</h4>
                        <ul>
                            <li><strong>M:</strong> Representa el margen (distancia m√≠nima al hiperplano)</li>
                            <li><strong>Restricci√≥n 1:</strong> Normalizaci√≥n de los coeficientes</li>
                            <li><strong>Restricci√≥n 2:</strong> Cada observaci√≥n debe estar en el lado correcto del margen</li>
                        </ul>
                    </div>
                </div>

                <div class="column">
                    <h3>Vectores de Soporte</h3>
                    <div id="maximal-margin-plot" class="plot-container"></div>
                    <div class="highlight-box">
                        <p>
                            Los <strong>vectores de soporte</strong> son las observaciones que:
                        </p>
                        <ul>
                            <li>‚úì Se encuentran exactamente sobre el margen</li>
                            <li>‚úì Son equidistantes del hiperplano de margen m√°ximo</li>
                            <li>‚úì "Soportan" el hiperplano (si se mueven, el hiperplano cambia)</li>
                        </ul>
                        <p class="note">
                            üí° <strong>Nota importante:</strong> S√≥lo los vectores de soporte afectan al clasificador.
                            Las observaciones alejadas del margen no influyen en la frontera de decisi√≥n.
                        </p>
                    </div>
                </div>
            </div>

            <!-- Visualizaci√≥n 3D del Margen M√°ximo -->
            <div class="interactive-box" style="margin-top: 3rem;">
                <h3>üé® Visualizaci√≥n 3D del Margen M√°ximo</h3>
                <p>Observa c√≥mo el hiperplano (plano 2D en este espacio 3D) separa dos clases maximizando el margen.</p>
                <div id="maximal-margin-3d-plot" class="plot-container-large"></div>
                <div class="highlight-box">
                    <h4>üîç Observa en la visualizaci√≥n 3D:</h4>
                    <ul>
                        <li><strong>Puntos azules y rojos:</strong> Las dos clases separadas</li>
                        <li><strong>Plano semi-transparente:</strong> El hiperplano de decisi√≥n</li>
                        <li><strong>Puntos marcados (c√≠rculos grandes):</strong> Los vectores de soporte</li>
                        <li><strong>Separaci√≥n m√°xima:</strong> El plano est√° equidistante de los puntos m√°s cercanos de cada clase</li>
                    </ul>
                </div>
            </div>

            <div class="limitation-box">
                <h3>‚ö†Ô∏è Limitaci√≥n Principal</h3>
                <p>
                    El clasificador de margen m√°ximo <strong>requiere que las clases sean linealmente separables</strong>.
                    En muchos casos reales, esto no es posible debido a:
                </p>
                <ul>
                    <li><strong>Ruido en los datos:</strong> Observaciones mal etiquetadas o con errores de medici√≥n</li>
                    <li><strong>Solapamiento natural:</strong> Las clases pueden tener regiones de superposici√≥n inherente</li>
                    <li><strong>Outliers:</strong> Puntos at√≠picos que hacen imposible la separaci√≥n perfecta</li>
                    <li><strong>Datos de alta dimensi√≥n:</strong> La separabilidad lineal es menos com√∫n en espacios de alta dimensi√≥n</li>
                </ul>
                <p style="margin-top: 1rem;">
                    Adem√°s, incluso cuando existe un hiperplano separador, el clasificador de margen m√°ximo puede ser
                    <strong>muy sensible a cambios peque√±os</strong> en las observaciones. Un solo outlier puede cambiar
                    dr√°sticamente el hiperplano √≥ptimo, llevando a un modelo con alta varianza.
                </p>
            </div>
        </div>
    </section>

    <!-- Support Vector Classifier -->
    <section id="svc" class="section bg-light">
        <div class="container">
            <h2 class="section-title">Support Vector Classifier (Soft Margin)</h2>

            <div class="intro-comparison">
                <div class="comparison-card">
                    <h4>‚ùå Problema con Margen M√°ximo</h4>
                    <ul>
                        <li>Requiere separaci√≥n perfecta</li>
                        <li>No funciona con datos solapados</li>
                        <li>Muy sensible a observaciones individuales</li>
                        <li>Puede sobre-ajustar los datos</li>
                    </ul>
                </div>
                <div class="comparison-arrow">‚Üí</div>
                <div class="comparison-card">
                    <h4>‚úÖ Soluci√≥n: Soft Margin</h4>
                    <ul>
                        <li>Permite violaciones al margen</li>
                        <li>Funciona con cualquier tipo de datos</li>
                        <li>Mayor robustez</li>
                        <li>Mejor generalizaci√≥n</li>
                    </ul>
                </div>
            </div>

            <h3>Formulaci√≥n Matem√°tica del Support Vector Classifier</h3>
            <p class="intro-text">
                El Support Vector Classifier (tambi√©n conocido como <strong>soft margin classifier</strong>)
                extiende el clasificador de margen m√°ximo permitiendo que algunas observaciones est√©n
                en el lado incorrecto del margen, o incluso del hiperplano. Esto se logra introduciendo
                <em>variables de holgura</em> (slack variables) \(\epsilon_i\).
            </p>
            <div class="formula-box">
                $$\text{maximizar}_{\beta_0,\beta_1,\ldots,\beta_p,\epsilon_1,\ldots,\epsilon_n, M} \quad M$$
                $$\text{sujeto a} \quad \sum_{j=1}^p \beta_j^2 = 1$$
                $$y_i(\beta_0 + \beta_1x_{i1} + \cdots + \beta_px_{ip}) \geq M(1-\epsilon_i), \quad \forall i = 1,\ldots,n$$
                $$\epsilon_i \geq 0, \quad \sum_{i=1}^n \epsilon_i \leq C$$
            </div>
            <div class="info-box">
                <h4>üìê Interpretaci√≥n de la Formulaci√≥n:</h4>
                <p>
                    El t√©rmino \(M(1-\epsilon_i)\) en la segunda restricci√≥n permite que una observaci√≥n est√©
                    del lado correcto del hiperplano pero violando el margen cuando \(0 < \epsilon_i \leq 1\),
                    o incluso del lado incorrecto cuando \(\epsilon_i > 1\).
                </p>
            </div>

            <div class="two-column">
                <div class="column">
                    <h3>Variables de Holgura (Œµ)</h3>
                    <div class="slack-explanation">
                        <div class="slack-case">
                            <strong>Œµ<sub>i</sub> = 0:</strong>
                            <p>La observaci√≥n est√° en el lado correcto del margen</p>
                        </div>
                        <div class="slack-case">
                            <strong>0 &lt; Œµ<sub>i</sub> &lt; 1:</strong>
                            <p>La observaci√≥n est√° en el lado correcto del hiperplano, pero viola el margen</p>
                        </div>
                        <div class="slack-case">
                            <strong>Œµ<sub>i</sub> &gt; 1:</strong>
                            <p>La observaci√≥n est√° en el lado incorrecto del hiperplano (mal clasificada)</p>
                        </div>
                    </div>
                </div>

                <div class="column">
                    <h3>Par√°metro de Coste C</h3>
                    <div class="c-parameter-box">
                        <p><strong>C</strong> controla el presupuesto para violaciones al margen:</p>

                        <div class="c-slider-container">
                            <label>Valor de C: <span id="c-value">1</span></label>
                            <input type="range" id="c-slider" min="0.01" max="100" step="0.01" value="1">
                        </div>

                        <div id="svc-plot" class="plot-container"></div>

                        <div class="c-effects">
                            <div class="effect-card">
                                <h4>C peque√±o ‚Üí Margen ancho</h4>
                                <ul>
                                    <li>Muchas violaciones permitidas</li>
                                    <li>Bajo sesgo, alta varianza</li>
                                    <li>Muchos vectores de soporte</li>
                                </ul>
                            </div>
                            <div class="effect-card">
                                <h4>C grande ‚Üí Margen estrecho</h4>
                                <ul>
                                    <li>Pocas violaciones permitidas</li>
                                    <li>Alto sesgo, baja varianza</li>
                                    <li>Pocos vectores de soporte</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Support Vector Machine -->
    <section id="svm" class="section">
        <div class="container">
            <h2 class="section-title">Support Vector Machine (SVM)</h2>

            <div class="svm-intro">
                <p class="large-text">
                    ¬øQu√© hacemos cuando la frontera de decisi√≥n es <strong>no lineal</strong>?
                    ¬°Usamos <em>kernels</em> para ampliar el espacio de caracter√≠sticas!
                </p>
            </div>

            <h3>El Concepto de Kernel</h3>
            <div class="kernel-explanation">
                <p class="intro-text">
                    Un <strong>kernel</strong> es una funci√≥n que cuantifica la similitud entre dos observaciones.
                    La idea fundamental del <em>kernel trick</em> es que podemos calcular productos internos en
                    espacios de caracter√≠sticas de alta dimensi√≥n (incluso infinita) sin tener que calcular
                    expl√≠citamente las coordenadas en ese espacio.
                </p>
                <p class="intro-text">
                    Matem√°ticamente, el Support Vector Classifier se puede representar como:
                </p>
                <div class="formula-box">
                    $$f(x) = \beta_0 + \sum_{i \in \mathcal{S}} \alpha_i \langle x, x_i \rangle$$
                </div>
                <p class="intro-text">
                    donde \(\mathcal{S}\) es el conjunto de √≠ndices de los vectores de soporte, y \(\langle x, x_i \rangle\)
                    es el producto interno. Para obtener un SVM, simplemente reemplazamos el producto interno con un kernel:
                </p>
                <div class="formula-box">
                    $$f(x) = \beta_0 + \sum_{i \in \mathcal{S}} \alpha_i K(x, x_i)$$
                </div>
                <p class="intro-text">
                    Esto significa que para ajustar un SVM, solo necesitamos calcular \(K(x_i, x_{i'})\) para todos los
                    \(\binom{n}{2}\) pares distintos de observaciones de entrenamiento. ¬°No necesitamos trabajar expl√≠citamente
                    en el espacio ampliado de caracter√≠sticas!
                </p>
            </div>

            <div class="kernel-types">
                <h3>Tipos de Kernels</h3>

                <div class="kernel-grid">
                    <div class="kernel-card">
                        <h4>üîπ Kernel Lineal</h4>
                        <div class="formula-box-small">
                            $$K(x_i, x_{i'}) = \sum_{j=1}^p x_{ij}x_{i'j}$$
                        </div>
                        <p>Es simplemente el producto interno est√°ndar. Equivalente al Support Vector Classifier.</p>
                        <div class="kernel-viz" id="linear-kernel"></div>
                    </div>

                    <div class="kernel-card">
                        <h4>üî∏ Kernel Polinomial</h4>
                        <div class="formula-box-small">
                            $$K(x_i, x_{i'}) = \left(1 + \sum_{j=1}^p x_{ij}x_{i'j}\right)^d$$
                        </div>
                        <p>Donde <strong>d</strong> es un entero positivo (grado del polinomio).</p>
                        <div class="controls">
                            <label>Grado (d):
                                <input type="range" id="poly-degree" min="1" max="5" value="2">
                                <span id="poly-degree-value">2</span>
                            </label>
                        </div>
                        <div class="kernel-viz" id="poly-kernel"></div>
                    </div>

                    <div class="kernel-card">
                        <h4>üî¥ Kernel Radial (RBF)</h4>
                        <div class="formula-box-small">
                            $$K(x_i, x_{i'}) = \exp\left(-\gamma \sum_{j=1}^p (x_{ij} - x_{i'j})^2\right)$$
                        </div>
                        <p>Donde <strong>Œ≥</strong> es una constante positiva. Tiene comportamiento muy local.</p>
                        <div class="controls">
                            <label>Gamma (Œ≥):
                                <input type="range" id="gamma" min="0.001" max="10" step="0.001" value="1">
                                <span id="gamma-value">1</span>
                            </label>
                        </div>
                        <div class="kernel-viz" id="rbf-kernel"></div>
                    </div>
                </div>
            </div>

            <div class="kernel-comparison">
                <h3>Comparaci√≥n Interactiva de Kernels</h3>
                <div id="kernel-comparison-plot" class="plot-container-large"></div>
                <div class="kernel-selector">
                    <button class="kernel-btn active" data-kernel="linear">Lineal</button>
                    <button class="kernel-btn" data-kernel="poly">Polinomial</button>
                    <button class="kernel-btn" data-kernel="rbf">RBF</button>
                </div>
            </div>

            <div class="advantage-box">
                <h3>üí° Ventajas del Enfoque por Kernels</h3>
                <ul>
                    <li><strong>Eficiencia computacional:</strong> Solo necesitamos calcular \(K(x_i, x_{i'})\) para cada par de observaciones - no necesitamos calcular \(\phi(x)\) expl√≠citamente</li>
                    <li><strong>Flexibilidad:</strong> Podemos trabajar en espacios de caracter√≠sticas de dimensi√≥n infinita (como con el kernel RBF)</li>
                    <li><strong>Simplicidad:</strong> No necesitamos transformar expl√≠citamente los datos - el kernel lo hace impl√≠citamente</li>
                    <li><strong>Generalidad:</strong> El mismo algoritmo funciona para cualquier kernel v√°lido</li>
                </ul>
            </div>

            <!-- Visualizaci√≥n 3D de Kernels -->
            <div class="interactive-box" style="margin-top: 3rem;">
                <h3>üé® Visualizaci√≥n 3D: Efecto de los Kernels</h3>
                <p>Esta visualizaci√≥n muestra c√≥mo diferentes kernels transforman un problema no lineal en uno linealmente separable en un espacio de mayor dimensi√≥n.</p>
                <div id="kernel-3d-plot" class="plot-container-large"></div>
                <div class="controls">
                    <label>
                        Seleccionar Kernel:
                        <select id="kernel-3d-selector">
                            <option value="linear">Lineal (no transforma)</option>
                            <option value="poly" selected>Polinomial</option>
                            <option value="rbf">RBF (Radial)</option>
                        </select>
                    </label>
                </div>
                <div class="highlight-box">
                    <h4>üîç Interpretaci√≥n:</h4>
                    <ul>
                        <li><strong>Kernel Lineal:</strong> Los datos se mantienen en el mismo espacio - no hay transformaci√≥n</li>
                        <li><strong>Kernel Polinomial:</strong> Proyecta los datos a un espacio de mayor dimensi√≥n donde se vuelven separables</li>
                        <li><strong>Kernel RBF:</strong> Crea una representaci√≥n en un espacio de dimensi√≥n infinita (mostrado aqu√≠ de forma aproximada)</li>
                    </ul>
                </div>
            </div>
        </div>
    </section>

    <!-- Ejemplos Interactivos -->
    <section id="ejemplos" class="section bg-light">
        <div class="container">
            <h2 class="section-title">Ejemplos Interactivos</h2>

            <div class="example-container">
                <h3>üéÆ Generador de Datos y Clasificaci√≥n</h3>
                <p>Genera tus propios datos y observa c√≥mo diferentes SVM los clasifican.</p>

                <div class="data-generator">
                    <div class="generator-controls">
                        <h4>Configuraci√≥n de Datos</h4>
                        <label>
                            Tipo de patr√≥n:
                            <select id="pattern-type">
                                <option value="linear">Lineal separable</option>
                                <option value="overlap">Con solapamiento</option>
                                <option value="circular">Circular</option>
                                <option value="xor">XOR (no lineal)</option>
                            </select>
                        </label>
                        <label>
                            N√∫mero de puntos:
                            <input type="number" id="n-points" min="20" max="500" value="100">
                        </label>
                        <label>
                            Ruido:
                            <input type="range" id="noise" min="0" max="2" step="0.1" value="0.3">
                            <span id="noise-value">0.3</span>
                        </label>
                        <button class="btn-primary" id="generate-data">Generar Datos</button>
                    </div>

                    <div class="classifier-controls">
                        <h4>Configuraci√≥n del Clasificador</h4>
                        <label>
                            Kernel:
                            <select id="classifier-kernel">
                                <option value="linear">Lineal</option>
                                <option value="poly">Polinomial</option>
                                <option value="rbf">RBF</option>
                            </select>
                        </label>
                        <label>
                            Par√°metro C:
                            <input type="range" id="classifier-c" min="0.001" max="100" step="0.001" value="1">
                            <span id="classifier-c-value">1</span>
                        </label>
                        <div id="kernel-params"></div>
                        <button class="btn-success" id="train-classifier">Entrenar SVM</button>
                    </div>
                </div>

                <div id="interactive-plot" class="plot-container-large"></div>

                <div class="results-panel">
                    <h4>Resultados</h4>
                    <div class="metrics">
                        <div class="metric">
                            <span class="metric-label">Vectores de Soporte:</span>
                            <span class="metric-value" id="n-support-vectors">-</span>
                        </div>
                        <div class="metric">
                            <span class="metric-label">Precisi√≥n:</span>
                            <span class="metric-value" id="accuracy">-</span>
                        </div>
                        <div class="metric">
                            <span class="metric-label">Ancho del Margen:</span>
                            <span class="metric-value" id="margin-width">-</span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- SVMs con M√°s de Dos Clases -->
    <section id="multiclass" class="section bg-light">
        <div class="container">
            <h2 class="section-title">SVMs con M√°s de Dos Clases</h2>

            <div class="concept-intro">
                <p class="intro-text">
                    Hasta ahora hemos discutido SVMs para clasificaci√≥n binaria (dos clases). Sin embargo, en muchas
                    aplicaciones del mundo real tenemos <strong>K > 2 clases</strong>. Aunque el concepto de hiperplanos
                    separadores no se extiende naturalmente a m√°s de dos clases, existen varias propuestas para adaptar
                    los SVMs a este escenario.
                </p>
                <p class="intro-text">
                    Las dos aproximaciones m√°s populares son: <strong>One-Versus-One (OVO)</strong> y
                    <strong>One-Versus-All (OVA)</strong>, tambi√©n conocida como One-Versus-Rest.
                </p>
            </div>

            <div class="two-column" style="margin-top: 3rem;">
                <div class="column">
                    <div class="method-card">
                        <h3>üîµ One-Versus-One (OVO)</h3>
                        <h4>Tambi√©n conocido como: All-Pairs</h4>

                        <div class="formula-box">
                            $$\text{N√∫mero de clasificadores} = \binom{K}{2} = \frac{K(K-1)}{2}$$
                        </div>

                        <div class="key-points">
                            <h4>üìã C√≥mo Funciona:</h4>
                            <ol style="text-align: left; line-height: 1.8;">
                                <li>Se construyen $\binom{K}{2}$ SVMs, cada uno comparando un par de clases</li>
                                <li>Por ejemplo, un SVM compara la clase k (codificada como +1) con la clase k' (codificada como -1)</li>
                                <li>Para clasificar una nueva observaci√≥n de prueba:
                                    <ul style="margin-top: 0.5rem;">
                                        <li>Se usa cada uno de los $\binom{K}{2}$ clasificadores</li>
                                        <li>Se cuenta cu√°ntas veces se asigna a cada clase</li>
                                        <li>Se asigna a la clase que recibi√≥ m√°s votos</li>
                                    </ul>
                                </li>
                            </ol>
                        </div>

                        <div class="highlight-box" style="margin-top: 1rem;">
                            <h4>üí° Ejemplo con K=4 clases:</h4>
                            <p>Se necesitan $\binom{4}{2} = 6$ clasificadores:</p>
                            <ul>
                                <li>Clase 1 vs Clase 2</li>
                                <li>Clase 1 vs Clase 3</li>
                                <li>Clase 1 vs Clase 4</li>
                                <li>Clase 2 vs Clase 3</li>
                                <li>Clase 2 vs Clase 4</li>
                                <li>Clase 3 vs Clase 4</li>
                            </ul>
                        </div>

                        <div class="info-box" style="margin-top: 1rem;">
                            <h4>‚úÖ Ventajas:</h4>
                            <ul>
                                <li>Cada clasificador solo necesita datos de dos clases</li>
                                <li>M√°s eficiente computacionalmente para cada clasificador individual</li>
                                <li>Funciona bien en la pr√°ctica</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="column">
                    <div class="method-card">
                        <h3>üü¢ One-Versus-All (OVA)</h3>
                        <h4>Tambi√©n conocido como: One-Versus-Rest</h4>

                        <div class="formula-box">
                            $$\text{N√∫mero de clasificadores} = K$$
                        </div>

                        <div class="key-points">
                            <h4>üìã C√≥mo Funciona:</h4>
                            <ol style="text-align: left; line-height: 1.8;">
                                <li>Se construyen K SVMs, uno para cada clase</li>
                                <li>El k-√©simo SVM compara la clase k (codificada como +1) contra todas las dem√°s K-1 clases (codificadas como -1)</li>
                                <li>Sean Œ≤<sub>0k</sub>, Œ≤<sub>1k</sub>, ..., Œ≤<sub>pk</sub> los par√°metros del k-√©simo SVM</li>
                                <li>Para clasificar una observaci√≥n de prueba x*:
                                    <ul style="margin-top: 0.5rem;">
                                        <li>Se calcula f<sub>k</sub>(x*) = Œ≤<sub>0k</sub> + Œ≤<sub>1k</sub>x<sub>1</sub>* + ... + Œ≤<sub>pk</sub>x<sub>p</sub>* para cada k</li>
                                        <li>Se asigna a la clase con el valor m√°s alto de f<sub>k</sub>(x*)</li>
                                    </ul>
                                </li>
                            </ol>
                        </div>

                        <div class="highlight-box" style="margin-top: 1rem;">
                            <h4>üí° Ejemplo con K=4 clases:</h4>
                            <p>Se necesitan K = 4 clasificadores:</p>
                            <ul>
                                <li>Clase 1 vs {Clases 2, 3, 4}</li>
                                <li>Clase 2 vs {Clases 1, 3, 4}</li>
                                <li>Clase 3 vs {Clases 1, 2, 4}</li>
                                <li>Clase 4 vs {Clases 1, 2, 3}</li>
                            </ul>
                        </div>

                        <div class="info-box" style="margin-top: 1rem;">
                            <h4>‚úÖ Ventajas:</h4>
                            <ul>
                                <li>Menos clasificadores que OVO cuando K es grande</li>
                                <li>Interpretaci√≥n clara: nivel de confianza de pertenencia a cada clase</li>
                                <li>M√°s simple conceptualmente</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Visualizaci√≥n Interactiva de Clasificaci√≥n Multiclase -->
            <div class="interactive-box" style="margin-top: 3rem;">
                <h3>üé® Visualizaci√≥n: Clasificaci√≥n Multiclase con SVM</h3>
                <p>Esta visualizaci√≥n muestra c√≥mo funcionan los SVMs con 3 clases usando diferentes estrategias.</p>

                <div class="controls" style="margin-bottom: 2rem;">
                    <label style="margin-right: 2rem;">
                        Estrategia:
                        <select id="multiclass-strategy">
                            <option value="ovo">One-Versus-One (OVO)</option>
                            <option value="ovr">One-Versus-All (OVR)</option>
                        </select>
                    </label>
                    <button class="btn-primary" id="generate-multiclass">Generar Nuevo Dataset</button>
                </div>

                <div id="multiclass-plot" class="plot-container-large"></div>

                <div class="results-panel" style="margin-top: 1.5rem;">
                    <h4>Informaci√≥n del Clasificador</h4>
                    <div class="metrics">
                        <div class="metric">
                            <span class="metric-label">N√∫mero de Clases:</span>
                            <span class="metric-value">3</span>
                        </div>
                        <div class="metric">
                            <span class="metric-label">Clasificadores Entrenados:</span>
                            <span class="metric-value" id="num-classifiers">-</span>
                        </div>
                        <div class="metric">
                            <span class="metric-label">Precisi√≥n del Entrenamiento:</span>
                            <span class="metric-value" id="multiclass-accuracy">-</span>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Comparaci√≥n de Enfoques -->
            <div class="key-points" style="margin-top: 3rem;">
                <h3>‚öñÔ∏è Comparaci√≥n: OVO vs OVA</h3>
                <div class="comparison-table">
                    <table style="width: 100%; border-collapse: collapse; margin-top: 1rem;">
                        <thead>
                            <tr style="background: #1e293b; color: white;">
                                <th style="padding: 1rem; text-align: left; border: 1px solid #cbd5e1;">Aspecto</th>
                                <th style="padding: 1rem; text-align: left; border: 1px solid #cbd5e1;">One-Versus-One (OVO)</th>
                                <th style="padding: 1rem; text-align: left; border: 1px solid #cbd5e1;">One-Versus-All (OVA)</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td style="padding: 0.75rem; border: 1px solid #cbd5e1;"><strong>N√∫mero de clasificadores</strong></td>
                                <td style="padding: 0.75rem; border: 1px solid #cbd5e1;">$\binom{K}{2} = \frac{K(K-1)}{2}$</td>
                                <td style="padding: 0.75rem; border: 1px solid #cbd5e1;">K</td>
                            </tr>
                            <tr style="background: #f8fafc;">
                                <td style="padding: 0.75rem; border: 1px solid #cbd5e1;"><strong>Datos por clasificador</strong></td>
                                <td style="padding: 0.75rem; border: 1px solid #cbd5e1;">Solo 2 clases (m√°s peque√±o)</td>
                                <td style="padding: 0.75rem; border: 1px solid #cbd5e1;">Todas las clases (m√°s grande)</td>
                            </tr>
                            <tr>
                                <td style="padding: 0.75rem; border: 1px solid #cbd5e1;"><strong>Tiempo de entrenamiento</strong></td>
                                <td style="padding: 0.75rem; border: 1px solid #cbd5e1;">M√°s clasificadores pero cada uno es r√°pido</td>
                                <td style="padding: 0.75rem; border: 1px solid #cbd5e1;">Menos clasificadores pero cada uno es m√°s lento</td>
                            </tr>
                            <tr style="background: #f8fafc;">
                                <td style="padding: 0.75rem; border: 1px solid #cbd5e1;"><strong>Decisi√≥n final</strong></td>
                                <td style="padding: 0.75rem; border: 1px solid #cbd5e1;">Por votaci√≥n mayoritaria</td>
                                <td style="padding: 0.75rem; border: 1px solid #cbd5e1;">Por m√°ximo valor de f(x)</td>
                            </tr>
                            <tr>
                                <td style="padding: 0.75rem; border: 1px solid #cbd5e1;"><strong>Desbalance de clases</strong></td>
                                <td style="padding: 0.75rem; border: 1px solid #cbd5e1;">Menos afectado</td>
                                <td style="padding: 0.75rem; border: 1px solid #cbd5e1;">M√°s afectado</td>
                            </tr>
                            <tr style="background: #f8fafc;">
                                <td style="padding: 0.75rem; border: 1px solid #cbd5e1;"><strong>Uso en pr√°ctica</strong></td>
                                <td style="padding: 0.75rem; border: 1px solid #cbd5e1;">Muy popular, implementaci√≥n por defecto en muchas bibliotecas</td>
                                <td style="padding: 0.75rem; border: 1px solid #cbd5e1;">Tambi√©n popular, conceptualmente m√°s simple</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <div class="highlight-box" style="margin-top: 2rem;">
                <h3>üéØ Recomendaciones Pr√°cticas</h3>
                <ul class="key-list">
                    <li><strong>Usar OVO cuando:</strong> Tienes muchas clases (K grande) y prefieres clasificadores m√°s simples y r√°pidos de entrenar individualmente</li>
                    <li><strong>Usar OVA cuando:</strong> Tienes pocas clases (K peque√±o) o quieres interpretaci√≥n directa de la confianza de cada clase</li>
                    <li><strong>En la pr√°ctica:</strong> Ambos m√©todos suelen dar resultados similares. Muchas bibliotecas (como scikit-learn) usan OVO por defecto</li>
                    <li><strong>Dato importante:</strong> Para K=3 clases, OVO requiere 3 clasificadores mientras que OVA tambi√©n requiere 3, pero para K=10, OVO necesita 45 y OVA solo 10</li>
                </ul>
            </div>
        </div>
    </section>

    <!-- Relaci√≥n con Regresi√≥n Log√≠stica -->
    <section class="section">
        <div class="container">
            <h2 class="section-title">Relaci√≥n con Regresi√≥n Log√≠stica</h2>

            <div class="comparison-intro">
                <p>
                    El Support Vector Classifier se puede reformular como un problema de optimizaci√≥n
                    de la forma "P√©rdida + Penalizaci√≥n":
                </p>
            </div>

            <div class="formula-box">
                $$\min_{\beta_0,\beta_1,\ldots,\beta_p} \left\{ \sum_{i=1}^n \max[0, 1 - y_if(x_i)] + \lambda \sum_{j=1}^p \beta_j^2 \right\}$$
            </div>

            <div class="two-column">
                <div class="column">
                    <h3>Funci√≥n de P√©rdida Hinge</h3>
                    <div id="loss-comparison-plot" class="plot-container"></div>
                    <p>
                        La p√©rdida del SVM (hinge loss) es similar a la de regresi√≥n log√≠stica,
                        pero tiene valor exactamente cero para observaciones bien clasificadas.
                    </p>
                </div>

                <div class="column">
                    <h3>Similitudes y Diferencias</h3>
                    <div class="comparison-table">
                        <table>
                            <thead>
                                <tr>
                                    <th>Aspecto</th>
                                    <th>SVM</th>
                                    <th>Reg. Log√≠stica</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Funci√≥n de p√©rdida</td>
                                    <td>Hinge loss</td>
                                    <td>Log loss</td>
                                </tr>
                                <tr>
                                    <td>Observaciones importantes</td>
                                    <td>Solo vectores de soporte</td>
                                    <td>Todas (pero menos peso a alejadas)</td>
                                </tr>
                                <tr>
                                    <td>Kernels</td>
                                    <td>Muy comunes</td>
                                    <td>Menos comunes</td>
                                </tr>
                                <tr>
                                    <td>Clases bien separadas</td>
                                    <td>Mejor desempe√±o</td>
                                    <td>Desempe√±o comparable</td>
                                </tr>
                                <tr>
                                    <td>Clases solapadas</td>
                                    <td>Comparable</td>
                                    <td>A veces preferible</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Aplicaciones Pr√°cticas -->
    <section class="section bg-light">
        <div class="container">
            <h2 class="section-title">Aplicaciones Pr√°cticas de SVM</h2>

            <p class="intro-text" style="text-align: center; margin-bottom: 3rem;">
                Las Support Vector Machines han demostrado ser extremadamente vers√°tiles y efectivas en una amplia
                variedad de dominios. Su capacidad para manejar datos de alta dimensi√≥n, junto con el uso de kernels
                para capturar relaciones no lineales, las hace ideales para muchos problemas del mundo real.
            </p>

            <div class="applications-grid">
                <div class="app-card">
                    <div class="app-icon">üè•</div>
                    <h3>Diagn√≥stico M√©dico</h3>
                    <p>Los SVM son especialmente √∫tiles en bioinform√°tica debido a su capacidad de manejar datos de alta dimensi√≥n con pocas muestras, com√∫n en estudios gen√≥micos.</p>
                    <ul>
                        <li><strong>Detecci√≥n de c√°ncer:</strong> Clasificaci√≥n basada en perfiles de expresi√≥n gen√©tica</li>
                        <li><strong>Diagn√≥stico de diabetes:</strong> Predicci√≥n usando datos cl√≠nicos y de laboratorio</li>
                        <li><strong>Clasificaci√≥n de tumores:</strong> Diferenciaci√≥n entre tipos de c√°ncer</li>
                        <li><strong>Im√°genes m√©dicas:</strong> Detecci√≥n de anomal√≠as en radiograf√≠as y resonancias</li>
                    </ul>
                </div>

                <div class="app-card">
                    <div class="app-icon">üìù</div>
                    <h3>Procesamiento de Texto</h3>
                    <p>Los SVM con kernel lineal funcionan excepcionalmente bien en problemas de clasificaci√≥n de texto debido a la alta dimensionalidad y dispersi√≥n de los datos.</p>
                    <ul>
                        <li><strong>OCR:</strong> Reconocimiento de caracteres escritos a mano (dataset MNIST)</li>
                        <li><strong>Clasificaci√≥n de documentos:</strong> Categorizaci√≥n autom√°tica de art√≠culos</li>
                        <li><strong>Detecci√≥n de spam:</strong> Filtrado de correos no deseados</li>
                        <li><strong>An√°lisis de sentimientos:</strong> Clasificaci√≥n de opiniones positivas/negativas</li>
                    </ul>
                </div>

                <div class="app-card">
                    <div class="app-icon">üñºÔ∏è</div>
                    <h3>Visi√≥n por Computadora</h3>
                    <p>Los SVM, especialmente con kernels RBF, han sido fundamentales en el desarrollo de sistemas de visi√≥n artificial antes del auge del deep learning.</p>
                    <ul>
                        <li><strong>Reconocimiento facial:</strong> Identificaci√≥n y verificaci√≥n de personas</li>
                        <li><strong>Clasificaci√≥n de objetos:</strong> Detecci√≥n de peatones, veh√≠culos, etc.</li>
                        <li><strong>Segmentaci√≥n de im√°genes:</strong> Separaci√≥n de regiones en im√°genes m√©dicas</li>
                        <li><strong>Detecci√≥n de gestos:</strong> Interfaces de interacci√≥n humano-computadora</li>
                    </ul>
                </div>

                <div class="app-card">
                    <div class="app-icon">üìä</div>
                    <h3>Finanzas y Econom√≠a</h3>
                    <p>Los SVM proporcionan modelos robustos para toma de decisiones financieras, especialmente cuando los datos son ruidosos o no linealmente separables.</p>
                    <ul>
                        <li><strong>Credit scoring:</strong> Evaluaci√≥n de riesgo crediticio de clientes</li>
                        <li><strong>Detecci√≥n de fraudes:</strong> Identificaci√≥n de transacciones sospechosas</li>
                        <li><strong>Predicci√≥n de mercados:</strong> Tendencias en precios de acciones</li>
                        <li><strong>An√°lisis de riesgo:</strong> Evaluaci√≥n de portafolios de inversi√≥n</li>
                    </ul>
                </div>

                <div class="app-card">
                    <div class="app-icon">üî¨</div>
                    <h3>Investigaci√≥n Cient√≠fica</h3>
                    <p>Los SVM se utilizan ampliamente en investigaci√≥n debido a su fundamento te√≥rico s√≥lido y capacidad de generalizaci√≥n.</p>
                    <ul>
                        <li><strong>Bioinform√°tica:</strong> Predicci√≥n de estructura de prote√≠nas</li>
                        <li><strong>Qu√≠mica computacional:</strong> Relaciones estructura-actividad (QSAR)</li>
                        <li><strong>Ecolog√≠a:</strong> Modelado de distribuci√≥n de especies</li>
                        <li><strong>F√≠sica de part√≠culas:</strong> Clasificaci√≥n de eventos en aceleradores</li>
                    </ul>
                </div>

                <div class="app-card">
                    <div class="app-icon">ü§ñ</div>
                    <h3>Ingenier√≠a y Tecnolog√≠a</h3>
                    <p>Aplicaciones en sistemas industriales donde la robustez y precisi√≥n son cr√≠ticas.</p>
                    <ul>
                        <li><strong>Control de calidad:</strong> Detecci√≥n de defectos en manufactura</li>
                        <li><strong>Mantenimiento predictivo:</strong> Predicci√≥n de fallas en maquinaria</li>
                        <li><strong>Reconocimiento de voz:</strong> Clasificaci√≥n de fonemas</li>
                        <li><strong>Sistemas de recomendaci√≥n:</strong> Filtrado colaborativo</li>
                    </ul>
                </div>
            </div>

            <div class="key-points" style="margin-top: 3rem;">
                <h3>üéØ ¬øCu√°ndo usar SVM?</h3>
                <ul class="key-list">
                    <li><strong>Datos de alta dimensi√≥n:</strong> Cuando p es grande relativo a n (ej. clasificaci√≥n de texto, gen√≥mica)</li>
                    <li><strong>Fronteras de decisi√≥n complejas:</strong> Cuando las clases tienen patrones no lineales que pueden capturarse con kernels</li>
                    <li><strong>Datos limpios o semi-limpios:</strong> SVM es robusto pero funciona mejor sin demasiados outliers extremos</li>
                    <li><strong>Problemas de clasificaci√≥n binaria:</strong> Aunque se puede extender a multiclase, SVM brilla en problemas de dos clases</li>
                    <li><strong>Conjuntos de datos peque√±os a medianos:</strong> Con n entre cientos y decenas de miles (para datasets masivos, considerar SGD)</li>
                </ul>
            </div>
        </div>
    </section>

    <!-- Consejos Pr√°cticos -->
    <section class="section">
        <div class="container">
            <h2 class="section-title">Consejos Pr√°cticos para Usar SVM</h2>

            <div class="tips-grid">
                <div class="tip-card">
                    <h3>üìè Normalizaci√≥n de Datos</h3>
                    <p>
                        <strong>Siempre normaliza tus caracter√≠sticas</strong> antes de aplicar SVM.
                        Los SVM son sensibles a la escala debido a que calculan distancias.
                    </p>
                    <div class="code-example">
                        <code>
from sklearn.preprocessing import StandardScaler<br>
scaler = StandardScaler()<br>
X_scaled = scaler.fit_transform(X)
                        </code>
                    </div>
                </div>

                <div class="tip-card">
                    <h3>üéØ Selecci√≥n de Kernel</h3>
                    <p><strong>Gu√≠a de decisi√≥n:</strong></p>
                    <ul>
                        <li><strong>Kernel lineal:</strong> Cuando p es grande relativo a n</li>
                        <li><strong>Kernel RBF:</strong> Cuando n es grande y p es peque√±o</li>
                        <li><strong>Kernel polinomial:</strong> Para relaciones polinomiales espec√≠ficas</li>
                    </ul>
                </div>

                <div class="tip-card">
                    <h3>‚öôÔ∏è Ajuste de Hiperpar√°metros</h3>
                    <p>Usa <strong>validaci√≥n cruzada</strong> para seleccionar:</p>
                    <ul>
                        <li><strong>C:</strong> Controla el balance sesgo-varianza</li>
                        <li><strong>Œ≥ (para RBF):</strong> Controla la "localidad" del kernel</li>
                        <li><strong>d (para polinomial):</strong> Grado del polinomio</li>
                    </ul>
                </div>

                <div class="tip-card">
                    <h3>‚ö° Eficiencia Computacional</h3>
                    <p><strong>Para datasets grandes:</strong></p>
                    <ul>
                        <li>Considera usar LinearSVC para problemas lineales</li>
                        <li>Usa SGDClassifier con loss='hinge' para datos masivos</li>
                        <li>Reduce dimensionalidad primero si es posible</li>
                    </ul>
                </div>
            </div>
        </div>
    </section>

    <!-- Resumen -->
    <section class="section bg-dark">
        <div class="container">
            <h2 class="section-title-white">Resumen de Conceptos Clave</h2>

            <div class="summary-grid">
                <div class="summary-card">
                    <h4>Clasificador de Margen M√°ximo</h4>
                    <ul>
                        <li>‚úì Maximiza la distancia al hiperplano</li>
                        <li>‚úì Solo funciona con datos linealmente separables</li>
                        <li>‚úì Depende solo de vectores de soporte</li>
                        <li>‚úó Muy sensible a outliers</li>
                    </ul>
                </div>

                <div class="summary-card">
                    <h4>Support Vector Classifier</h4>
                    <ul>
                        <li>‚úì Permite violaciones al margen ("soft margin")</li>
                        <li>‚úì Funciona con datos no separables</li>
                        <li>‚úì Par√°metro C controla sesgo-varianza</li>
                        <li>‚úì Mayor robustez</li>
                    </ul>
                </div>

                <div class="summary-card">
                    <h4>Support Vector Machine</h4>
                    <ul>
                        <li>‚úì Usa kernels para fronteras no lineales</li>
                        <li>‚úì Computacionalmente eficiente</li>
                        <li>‚úì Puede trabajar en dimensiones infinitas</li>
                        <li>‚úì Muy vers√°til</li>
                    </ul>
                </div>
            </div>

            <div class="final-thoughts">
                <h3>üéì Conclusi√≥n</h3>
                <p class="large-text-white">
                    Las Support Vector Machines son una herramienta poderosa y elegante para clasificaci√≥n.
                    Su capacidad de encontrar fronteras de decisi√≥n √≥ptimas, combinada con el uso de kernels
                    para manejar relaciones no lineales, las hace extremadamente vers√°tiles. Aunque han sido
                    parcialmente eclipsadas por redes neuronales profundas en algunos dominios, siguen siendo
                    una excelente opci√≥n para muchos problemas pr√°cticos, especialmente cuando los datos no
                    son masivos y la interpretabilidad es importante.
                </p>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>&copy; 2024 SVM Learning - Basado en "An Introduction to Statistical Learning" por James, Witten, Hastie y Tibshirani</p>
            <p>P√°gina web interactiva creada con fines educativos</p>
        </div>
    </footer>

    <script src="script.js"></script>
</body>
</html>
